# 第七章 参数估计

统计推断的基本问题可以分为两大类，一类是估计问题，另一类是假设检验问题。本章讨论总体参数的点估计和区间估计。

## §1 点 估 计

设总体 $X$ 的分布函数的形式已知，但它的一个或多个参数未知，借助于总体 $X$ 的一个样本来估计总体未知参数的值的问题称为参数的点估计问题。

**例1** 在某炸药制造厂，一天中发生着火现象的次数 $X$ 是一个随机变量，假设它服从以 $\lambda > 0$ 为参数的泊松分布，参数 $\lambda$ 为未知。现有以下的样本值，试估计参数 $\lambda$。

| 着火次数 $k$ | 0   | 1   | 2   | 3   | 4   | 5   | 6   |
|---|---|---|---|---|---|---|---|
| 发生 $k$ 次着火的天数 $n_k$ | 75 | 90 | 54 | 22 | 6   | 2   | 1   |

**解** 由于 $X \sim \pi(\lambda)$，故有 $\lambda = E(X)$。我们自然想到用样本均值来估计总体的均值 $E(X)$。现由已知数据计算得到
$
\overline{x} = \frac{\sum_{k=0}^{6} kn_k}{\sum_{k=0}^{6} n_k}
$
$
= \frac{1}{250} [0 \times 75 + 1 \times 90 + 2 \times 54 + 3 \times 22 + 4 \times 6 + 5 \times 2 + 6 \times 1$ = 1.22,
$

即 $E(X) = \lambda$ 的估计为 1.22。

点估计问题的一般提法如下：设总体 $X$ 的分布函数 $F(x; \theta)$ 的形式为已知，$\theta$ 是待估参数。$X_1, X_2, \cdots, X_n$ 是 $X$ 的一个样本，$x_1, x_2, \cdots, x_n$ 是相应的一个样本值。点估计问题就是要构造一个适当的统计量 $\hat{\theta}(X_1, X_2, \cdots, X_n)$，用它的观察值 $\hat{\theta}(x_1, x_2, \cdots, x_n)$ 作为未知参数 $\theta$ 的近似值。我们称 $\hat{\theta}(X_1, X_2, \cdots, X_n)$ 为 $\theta$ 的估计量，称 $\hat{\theta}(x_1, x_2, \cdots, x_n)$ 为 $\theta$ 的估计值。在不致混淆的情况下统称估计量和估计量。

① 多于一个未知参数时，可同样讨论。

计值为估计，并都简记为 $\hat{\theta}$。由于估计量是样本的函数。因此对于不同的样本值，$\theta$ 的估计值一般是不相同的。

例如在例1中，我们用样本均值来估计总体均值。即有估计量
$
\hat{\lambda} = E(\hat{X}) = \frac{1}{n} \sum_{k=1}^{n} X_k, \quad n = 250.
$

估计值
$
\hat{\lambda} = E(\hat{X}) = \frac{1}{n} \sum_{k=1}^{n} x_k = 1.22.
$

下面介绍两种常用的构造估计量的方法：矩估计法和最大似然估计法。

### （一）矩估计法

设 $X$ 为连续型随机变量，其概率密度为 $f(x;\theta_1,\theta_2,\cdots,\theta_k)$，或 $X$ 为离散型随机变量，其分布律为 $P(X=x)=p(x;\theta_1,\theta_2,\cdots,\theta_k)$，其中 $\theta_1,\theta_2,\cdots,\theta_k$ 为待估参数，$X_1,X_2,\cdots,X_n$ 是来自 $X$ 的样本。假设总体 $X$ 的前 $k$ 阶矩
$
\mu_l = E(X^l) = \int_{-\infty}^{\infty} x^l f(x;\theta_1,\theta_2,\cdots,\theta_k) dx \quad (X \text{ 连续型})
$
或
$
\mu_l = E(X^l) = \sum_{x \in R_X} x^l p(x;\theta_1,\theta_2,\cdots,\theta_k) \quad (X \text{ 离散型})
$

(其中 $R_X$ 是 $X$ 可能取值的范围)存在。一般来说，它们是 $\theta_1,\theta_2,\cdots,\theta_k$ 的函数。基于样本矩
$
A_l = \frac{1}{n} \sum_{i=1}^{n} X_i
$

依概率收敛于相应的总体矩 $\mu_l(l=1,2,\cdots,k)$，样本矩的连续函数依概率收敛于相应的总体矩的连续函数(见第六章§3)，我们就用样本矩作为相应的总体矩的估计量，而以样本矩的连续函数作为相应的总体矩的连续函数的估计量。这种估计方法称为矩估计法。矩估计法的具体做法如下：设
$
\begin{cases}
\mu_1 = \mu_1 (\theta_1,\theta_2,\cdots,\theta_k), \\
\mu_2 = \mu_2 (\theta_1,\theta_2,\cdots,\theta_k), \\
\vdots \\
\mu_k = \mu_k (\theta_1,\theta_2,\cdots,\theta_k).
\end{cases}
$

这是一个包含 $k$ 个未知参数 $\theta_1,\theta_2,\cdots,\theta_k$ 的联立方程组。一般来说，可以从中解出 $\theta_1,\theta_2,\cdots,\theta_k$，得到
$
\begin{cases}
\theta_1 = \theta_1 (\mu_1,\mu_2,\cdots,\mu_k), \\
\theta_2 = \theta_2 (\mu_1,\mu_2,\cdots,\mu_k), \\
\vdots \\
\theta_k = \theta_k (\mu_1,\mu_2,\cdots,\mu_k).
\end{cases}
$

以 $ A_i $ 分别代替上式中的 $\mu_i, i=1,2,\cdots,k$，就以
$
\hat{\theta}_i = \theta_i (A_1,A_2,\cdots,A_k), i=1,2,\cdots,k
$
分别作为 $\theta_i, i=1,2,\cdots,k$ 的估计量，这种估计量称为矩估计量。矩估计量的观察值称为矩估计值。

**例 2** 设总体 $ X $ 在 $[a,b]$ 上服从均匀分布，$ a,b $ 未知。$ X_1,X_2,\cdots,X_n $ 是来自 $ X $ 的样本，试求 $ a,b $ 的矩估计量。

**解** 
$
\mu_1 = E(X) = (a+b)/2,
$
$
\mu_2 = E(X^2) = D(X) + [E(X)]^2
$
$
= (b-a)^2/12 + (a+b)^2/4.
$
即
$
\begin{cases}
a+b=2\mu_1,\\
b-a=\sqrt{12(\mu_2-\mu_1^2)}.
\end{cases}
$
解这一方程组得
$
a=\mu_1 - \sqrt{3(\mu_2-\mu_1^2)}, \quad b=\mu_1 + \sqrt{3(\mu_2-\mu_1^2)}.
$

分别以 $ A_1,A_2 $ 代替 $\mu_1,\mu_2$，得到 $ a,b $ 的矩估计量分别为（注意到 $\frac{1}{n} \sum_{i=1}^{n}X_i^2 - \overline{X}^2 = \frac{1}{n} \sum_{i=1}^{n}(X_i - \overline{X})^2$）：
$
\hat{a} = A_1 - \sqrt{3(A_2-A_1^2)} = \overline{X} - \sqrt{\frac{3}{n} \sum_{i=1}^{n}(X_i - \overline{X})^2},
$
$
\hat{b} = A_1 + \sqrt{3(A_2-A_1^2)} = \overline{X} + \sqrt{\frac{3}{n} \sum_{i=1}^{n}(X_i - \overline{X})^2}.
$

**例 3** 设总体 $ X $ 的均值 $\mu$ 及方差 $\sigma^2$ 都存在，且有 $\sigma^2>0$。但 $\mu,\sigma^2$ 均为未知。又设 $ X_1,X_2,\cdots,X_n $ 是来自 $ X $ 的样本。试求 $\mu,\sigma^2$ 的矩估计量。

**解** 
$
\begin{cases}
\mu_1 = E(X) = \mu,\\
\mu_2 = E(X^2) = D(X) + [E(X)]^2 = \sigma^2 + \mu^2.
\end{cases}
$
解得
$
\begin{cases}
\mu = \mu_1,\\
\sigma^2 = \mu_2 - \mu_1^2.
\end{cases}
$
分别以 $ A_1,A_2 $ 代替 $\mu_1,\mu_2$，得 $\mu$ 和 $\sigma^2$ 的矩估计量分别为
$
\hat{\mu} = A_1 = \overline{X},
$
$
\hat{\sigma}^2 = A_2 - A_1^2 = \frac{1}{n} \sum_{i=1}^{n}X_i^2 - \overline{X}^2 = \frac{1}{n} \sum_{i=1}^{n}(X_i - \overline{X})^2.
$

所得结果表明，总体均值与方差的矩估计量的表达式不因不同的总体分布而异。例如，$ X \sim N(\mu,\sigma^2),\mu,\sigma^2 $ 未知，即得 $\mu,\sigma^2$ 的矩估计量为

$
\hat{\mu} = \bar{X}, \quad \hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^{n}(X_i - \bar{X})^2.
$

### （二）最大似然估计法

若总体 $ X $ 属离散型，其分布律 $ P(X=x) = p(x;\theta), \theta \in \Theta $ 的形式为已知，$ \theta $ 为待估参数，$ \Theta $ 是 $ \theta $ 可能取值的范围。设 $ X_1, X_2, \cdots, X_n $ 是来自 $ X $ 的样本，则 $ X_1, X_2, \cdots, X_n $ 的联合分布律为
$
\prod_{i=1}^{n}p(x_i;\theta).
$
又设 $ x_1, x_2, \cdots, x_n $ 是相应于样本 $ X_1, X_2, \cdots, X_n $ 的一个样本值。易知样本 $ X_1, X_2, \cdots, X_n $ 取到观察值 $ x_1, x_2, \cdots, x_n $ 的概率，亦即事件 $\lbrace X_1 = x_1, X_2 = x_2, \cdots, X_n = x_n\rbrace$ 发生的概率为
$
L(\theta) = L(x_1, x_2, \cdots, x_n; \theta) = \prod_{i=1}^{n}p(x_i;\theta), \theta \in \Theta. ^{1.1}
$
这一概率随 $ \theta $ 的取值而变化，它是 $ \theta $ 的函数，$ L(\theta) $ 称为样本的似然函数（注意，这里 $ x_1, x_2, \cdots, x_n $ 是已知的样本值，它们都是常数）。

关于最大似然估计法，我们有以下的直观想法：现在已经取到样本值 $ x_1, x_2, \cdots, x_n $ 了，这表明取到这一样本值的概率 $ L(\theta) $ 比较大。我们当然不会考虑那些不能使样本 $ x_1, x_2, \cdots, x_n $ 出现的 $ \theta \in \Theta $ 作为 $ \theta $ 的估计，再者，如果已知当 $ \theta = \theta_0 \in \Theta $ 时使 $ L(\theta) $ 取得大值，而 $ \theta $ 中的其他 $ \theta $ 的值使 $ L(\theta) $ 取得小值，我们自然认为取 $ \theta_0 $ 作为未知参数 $ \theta $ 的估计值，较为合理。由费希尔 (R. A. Fisher) 引进的最大似然估计法，就是固定样本观察值 $ x_1, x_2, \cdots, x_n $，在 $ \theta $ 取值的可能范围 $ \Theta $ 内挑选使似然函数 $ L(x_1, x_2, \cdots, x_n; \theta) $ 达到最大的参数值 $ \hat{\theta} $，作为参数 $ \theta $ 的估计值。即取 $ \hat{\theta} $ 使
$
L(x_1, x_2, \cdots, x_n; \hat{\theta}) = \max_{\theta \in \Theta} L(x_1, x_2, \cdots, x_n; \theta). ^{1.2}
$
这样得到的 $ \hat{\theta} $ 与样本值 $ x_1, x_2, \cdots, x_n $ 有关，常记为 $ \hat{\theta}(x_1, x_2, \cdots, x_n) $，称为参数 $ \theta $ 的最大似然估计值，而相应的统计量 $ \hat{\theta}(X_1, X_2, \cdots, X_n) $ 称为参数 $ \theta $ 的最大似然估计量。

若总体 $ X $ 属连续型，其概率密度 $ f(x;\theta), \theta \in \Theta $ 的形式已知，$ \theta $ 为待估参数，$ \Theta $ 是 $ \theta $ 可能取值的范围。设 $ X_1, X_2, \cdots, X_n $ 是来自 $ X $ 的样本，则 $ X_1, X_2, \cdots, X_n $ 的联合密度为
$
\prod_{i=1}^{n}f(x_i;\theta).
$
设 $ x_1, x_2, \cdots, x_n $ 是相应于样本 $ X_1, X_2, \cdots, X_n $ 的一个样本值，则随机点 $ (X_1, X_2, \cdots, X_n) $ 落在点 $ (x_1, x_2, \cdots, x_n) $ 的邻域（边长分别为 $ dx_1, dx_2, \cdots, dx_n $ 的 n 维立方体）内的概率近似地为

$
\prod_{i=1}^{n} f(x_i; \theta) dx_i. ^{1.3}
$

其值随 $\theta$ 的取值而变化。与离散型的情况一样，我们取 $\theta$ 的估计值 $\hat{\theta}$ 使概率 (1.3) 取到最大值，但因子 $\prod_{i=1}^{n} dx_i$ 不随 $\theta$ 而变，故只需考虑函数

$
L(\theta) = L(x_1, x_2, \cdots, x_n; \theta) = \prod_{i=1}^{n} f(x_i; \theta) ^{1.4}
$

的最大值。这里 $L(\theta)$ 称为样本的似然函数。若

$
L(x_1, x_2, \cdots, x_n; \hat{\theta}) = \max_{\theta \in \Theta} L(x_1, x_2, \cdots, x_n; \theta),
$

则称 $\hat{\theta}(x_1, x_2, \cdots, x_n)$ 为 $\theta$ 的最大似然估计值，称 $\hat{\theta}(X_1, X_2, \cdots, X_n)$ 为 $\theta$ 的最大似然估计量。

这样，确定最大似然估计量的问题就归结为微分学中的求最大值的问题了。

在很多情形下，$p(x; \theta)$ 和 $f(x; \theta)$ 关于 $\theta$ 可微，这时 $\hat{\theta}$ 常可从方程

$
\frac{d}{d\theta} L(\theta) = 0 ^{1.5}
$

解得①。又因 $L(\theta)$ 与 $\ln L(\theta)$ 在同一 $\theta$ 处取到极值，因此，$\theta$ 的最大似然估计 $\hat{\theta}$ 也可以从方程

$
\frac{d}{d\theta} \ln L(\theta) = 0 ^{1.6}
$

求得，而从后一方程求解往往比较方便。(1.6) 称为对数似然方程。

**例 4** 设 $X \sim b(1, p)$, $X_1, X_2, \cdots, X_n$ 是来自 $X$ 的一个样本，试求参数 $p$ 的最大似然估计量。

**解** 设 $x_1, x_2, \cdots, x_n$ 是相应于样本 $X_1, X_2, \cdots, X_n$ 的一个样本值。$X$ 的分布律为

$
P(X=x) = p^x(1-p)^{1-x}, \quad x=0,1.
$

故似然函数为

$
L(p) = \prod_{i=1}^{n} p^{x_i}(1-p)^{1-x_i} = p^{\sum_{i=1}^{n} x_i}(1-p)^{n - \sum_{i=1}^{n} x_i},
$

而

$
\ln L(p) = (\sum_{i=1}^{n} x_i) \ln p + (n - \sum_{i=1}^{n} x_i) \ln (1-p),
$

令

$
\frac{d}{dp} \ln L(p) = \frac{\sum_{i=1}^{n} x_i}{p} - \frac{n - \sum_{i=1}^{n} x_i}{1-p} = 0,
$

解得

$
\hat{p} = \frac{1}{n} \sum_{i=1}^{n} x_i = \overline{x}.
$

① 这里没有提到 $L(\theta)$ 取最大值的充分条件，但对于具体的函数是容易讨论的。

$
\prod_{i=1}^{n} f(x_i; \theta) dx_i. ^{1.3}
$

其值随 $\theta$ 的取值而变化。与离散型的情况一样，我们取 $\theta$ 的估计值 $\hat{\theta}$ 使概率 (1.3) 取到最大值，但因子 $\prod_{i=1}^{n} dx_i$ 不随 $\theta$ 而变，故只需考虑函数

$
L(\theta) = L(x_1, x_2, \cdots, x_n; \theta) = \prod_{i=1}^{n} f(x_i; \theta) ^{1.4}
$

的最大值。这里 $L(\theta)$ 称为样本的似然函数。若

$
L(x_1, x_2, \cdots, x_n; \hat{\theta}) = \max_{\theta \in \Theta} L(x_1, x_2, \cdots, x_n; \theta),
$

则称 $\hat{\theta}(x_1, x_2, \cdots, x_n)$ 为 $\theta$ 的最大似然估计值，称 $\hat{\theta}(X_1, X_2, \cdots, X_n)$ 为 $\theta$ 的最大似然估计量。

这样，确定最大似然估计量的问题就归结为微分学中的求最大值的问题了。

在很多情形下，$p(x; \theta)$ 和 $f(x; \theta)$ 关于 $\theta$ 可微，这时 $\hat{\theta}$ 常可从方程

$
\frac{d}{d\theta} L(\theta) = 0 ^{1.5}
$

解得①。又因 $L(\theta)$ 与 $\ln L(\theta)$ 在同一 $\theta$ 处取到极值，因此，$\theta$ 的最大似然估计 $\hat{\theta}$ 也可以从方程

$
\frac{d}{d\theta} \ln L(\theta) = 0 ^{1.6}
$

求得，而从后一方程求解往往比较方便。(1.6) 称为对数似然方程。

**例 4** 设 $X \sim b(1, p)$。$X_1, X_2, \cdots, X_n$ 是来自 $X$ 的一个样本，试求参数 $p$ 的最大似然估计量。

**解** 设 $x_1, x_2, \cdots, x_n$ 是相应于样本 $X_1, X_2, \cdots, X_n$ 的一个样本值。$X$ 的分布律为

$
P(X=x) = p^x(1-p)^{1-x}, \quad x=0,1.
$

故似然函数为

$
L(p) = \prod_{i=1}^{n} p^{x_i}(1-p)^{1-x_i} = p^{\sum_{i=1}^{n} x_i}(1-p)^{n - \sum_{i=1}^{n} x_i},
$

而

$
\ln L(p) = (\sum_{i=1}^{n} x_i) \ln p + (n - \sum_{i=1}^{n} x_i) \ln (1-p),
$

令

$
\frac{d}{dp} \ln L(p) = \frac{\sum_{i=1}^{n} x_i}{p} - \frac{n - \sum_{i=1}^{n} x_i}{1-p} = 0,
$

解得

$
\hat{p} = \frac{1}{n} \sum_{i=1}^{n} x_i = \overline{x}.
$

① 这里没有提到 $L(\theta)$ 取最大值的充分条件，但对于具体的函数是容易讨论的。

解得 $ p $ 的最大似然估计值

$
\hat{p} = \frac{1}{n} \sum_{i=1}^{n} x_i = \bar{x}.
$

$ p $ 的最大似然估计量为

$
\hat{p} = \frac{1}{n} \sum_{i=1}^{n} X_i = \bar{X}.
$

我们看到这一估计量与矩估计量是相同的。 □

最大似然估计法也适用于分布中含多个未知参数 $\theta_1, \theta_2, \cdots, \theta_k$ 的情况。这时，似然函数 $ L $ 是这些未知参数的函数。分别令

$
\frac{\partial}{\partial \theta_i} L = 0, \, i = 1, 2, \cdots, k
$

或令

$
\frac{\partial}{\partial \theta_i} \ln L = 0, \, i = 1, 2, \cdots, k. ^{1.7}
$

解上述由 $ k $ 个方程组成的方程组，即可得到各未知参数 $\theta_i (i = 1, 2, \cdots, k)$ 的最大似然估计值 $\hat{\theta_i}$。(1.7)称为对数似然方程组。

**例5** 设 $X \sim N(\mu, \sigma^2)$, $\mu, \sigma^2$ 为未知参数, $x_1, x_2, \cdots, x_n$ 是来自 $X$ 的一个样本值。求 $\mu, \sigma^2$ 的最大似然估计量。

**解** $X$ 的概率密度为

$
f(x; \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma}} \exp \left[ -\frac{1}{2\sigma^2} (x - \mu)^2 \right],
$

似然函数为

$
L(\mu, \sigma^2) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma}} \exp \left[ -\frac{1}{2\sigma^2} (x_i - \mu)^2 \right]
$

$
= (2\pi)^{-\frac{n}{2}} (\sigma^2)^{-\frac{n}{2}} \exp \left[ -\frac{1}{2\sigma^2} \sum_{i=1}^{n} (x_i - \mu)^2 \right].
$

而

$
\ln L = -\frac{n}{2} \ln (2\pi) - \frac{n}{2} \ln \sigma^2 - \frac{1}{2\sigma^2} \sum_{i=1}^{n} (x_i - \mu)^2.
$

令

$
\begin{cases}
\frac{\partial}{\partial \mu} \ln L = \frac{1}{\sigma^2} \left( \sum_{i=1}^{n} x_i - n\mu \right) = 0, \\
\frac{\partial}{\partial \sigma^2} \ln L = -\frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2} \sum_{i=1}^{n} (x_i - \mu)^2 = 0.
\end{cases}
$

由前一式解得 $\hat{\mu} = \frac{1}{n} \sum_{i=1}^{n} x_i = \bar{x}$, 代入后一式得 $\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2$. 因此得 $\mu$ 和 $\sigma^2$ 的最大似然估计量分别为

$
\hat{\mu} = \bar{X}, \, \hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^{n} (X_i - \bar{X})^2.
$

它们与相应的矩估计量相同。 □

**例6** 设总体 $X$ 在 $[a,b]$ 上服从均匀分布，$a,b$ 未知，$x_1,x_2,\cdots,x_n$ 是一个样本值。试求 $a,b$ 的最大似然估计量。

**解** 记 $x_{(1)}=\min\lbrace x_1,x_2,\cdots,x_n\rbrace, x_{(n)}=\max\lbrace x_1,x_2,\cdots,x_n\rbrace$。$X$ 的概率密度是
$
f(x;a,b)=\begin{cases} 
\frac{1}{b-a}, & a \leq x \leq b, \\
0, & \text{其他}.
\end{cases}
$
似然函数为
$
L(a,b)=\begin{cases} 
\frac{1}{(b-a)^n}, & a \leq x_1,x_2,\cdots,x_n \leq b, \\
0, & \text{其他}.
\end{cases}
$
由于 $a \leq x_1,x_2,\cdots,x_n \leq b$，等价于 $a \leq x_{(1)}, x_{(n)} \leq b$。似然函数可写成
$
L(a,b)=\begin{cases} 
\frac{1}{(b-a)^n}, & a \leq x_{(1)}, b \geq x_{(n)}, \\
0, & \text{其他}.
\end{cases}
$
于是对于满足条件 $a \leq x_{(1)}, b \geq x_{(n)}$ 的任意 $a,b$ 有
$
L(a,b)=\frac{1}{(b-a)^n} \leq \frac{1}{(x_{(n)} - x_{(1)})^n}.
$
即 $L(a,b)$ 在 $a=x_{(1)}, b=x_{(n)}$ 时取到最大值 $(x_{(n)} - x_{(1)})^{-n}$。故 $a,b$ 的最大似然估计值为
$
\hat{a}=x_{(1)}=\min_{1 \leq i \leq n} x_i, \quad \hat{b}=x_{(n)}=\max_{1 \leq i \leq n} x_i.
$
$a,b$ 的最大似然估计量为
$
\hat{a}=\min_{1 \leq i \leq n} X_i, \quad \hat{b}=\max_{1 \leq i \leq n} X_i.
$

此外，最大似然估计具有下述性质：设 $\theta$ 的函数 $u=u(\theta), \theta \in \Theta$ 具有单值反函数 $\theta=\theta(u), u \in \mathbb{R}$。又假设 $\hat{\theta}$ 是 $X$ 的概率分布中参数 $\theta$ 的最大似然估计，则 $\hat{u}=u(\hat{\theta})$ 是 $u(\theta)$ 的最大似然估计。这一性质称为最大似然估计的不变性。

事实上，因为 $\hat{\theta}$ 是 $\theta$ 的最大似然估计，于是有
$
L(x_1,x_2,\cdots,x_n;\hat{\theta})=\max_{\theta \in \Theta} L(x_1,x_2,\cdots,x_n;\hat{\theta}),
$
其中 $x_1,x_2,\cdots,x_n$ 是 $X$ 的一个样本值，考虑到 $\hat{u}=u(\hat{\theta})$，且有 $\hat{\theta}=\theta(\hat{u})$，上述可写成
$
L(x_1,x_2,\cdots,x_n;\theta(\hat{u}))=\max_{u \in \mathbb{R}} L(x_1,x_2,\cdots,x_n;\theta(u)).
$

这就证明了 $\hat{u}=u(\hat{\theta})$ 是 $u(\theta)$ 的最大似然估计。

当总体分布中含有多个未知参数时，也具有上述性质。例如，在例5中已得到 $\sigma^2$ 的最大似然估计为
$
\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^{n} (X_i - \overline{X})^2.
$
函数 $u = u(\sigma^2) = \sqrt{\sigma^2}$ 有单值反函数 $\sigma^2 = u^2 (u \geq 0)$，根据上述性质，得到标准差 $\sigma$ 的最大似然估计为
$
\hat{\sigma} = \sqrt{\hat{\sigma}^2} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (X_i - \overline{X})^2}.
$

我们还要提到的是，对数似然方程（1.6）或对数似然方程组（1.7）除了一些简单的情况外，往往没有有限函数形式的解，这就需要用数值方法求近似解。常用的算法是牛顿-拉弗森（Newton-Raphson）算法，对于（1.7）有时也用拟牛顿算法，它们都是迭代算法，读者可参考有关的参考书。
